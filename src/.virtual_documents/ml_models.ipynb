import os

import altair as alt
import dataframe_image as dfi
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import shap
from catboost import CatBoostRegressor
from lightgbm.sklearn import LGBMRegressor
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.dummy import DummyRegressor
from sklearn.feature_selection import RFE, RFECV, SelectFromModel
from sklearn.impute import SimpleImputer
from sklearn.linear_model import Lasso, LassoCV, LogisticRegression, Ridge, RidgeCV
from sklearn.metrics import make_scorer, mean_squared_error, r2_score
from sklearn.model_selection import (
    GridSearchCV,
    RandomizedSearchCV,
    ShuffleSplit,
    cross_val_score,
    cross_validate,
    train_test_split,
)
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import (
    OneHotEncoder,
    OrdinalEncoder,
    PolynomialFeatures,
    StandardScaler,
)
from sklearn.svm import SVC, SVR
from sklearn.tree import DecisionTreeRegressor, export_graphviz

alt.data_transformers.disable_max_rows()
alt.renderers.enable("mimetype")
alt.data_transformers.enable("data_server")


df = pd.read_csv("../data/processed/clean_data.csv").dropna()
df.shape
df.head()
train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)

X_train, y_train = train_df.drop(columns=["bloom_doy"]), train_df["bloom_doy"]
X_test, y_test = test_df.drop(columns=["bloom_doy"]), test_df["bloom_doy"]


df.describe()


categorical_features = ["country"]
numeric_features = [
    "lat",
    "long",
    "alt",
    "tmax",
    "tmin",
    "prcp",
    "agdd_winter",
    "tmax_winter",
    "prcp_winter",
    "co2_percapita",
    "year",
]
drop_features = ["city", "country"]
target = "bloom_doy"


scaler = StandardScaler()
enc = OneHotEncoder(handle_unknown="ignore", sparse=False)

preprocessor = make_column_transformer(
    (StandardScaler(), numeric_features),  # scaling on numeric features
    (
        OneHotEncoder(handle_unknown="ignore"),
        categorical_features,
    ),  # OHE on categorical features
    ("drop", drop_features),  # drop features
)

preprocessor.fit_transform(X_train).shape


def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):
    """
    Returns mean and std of cross validation

    Parameters
    ----------
    model :
        scikit-learn model
    X_train : numpy array or pandas DataFrame
        X in the training data
    y_train :
        y in the training data

    Returns
    ----------
        pandas Series with mean scores from cross_validation
    """
    scores = cross_validate(model, X_train, y_train, **kwargs)
    mean_scores = pd.DataFrame(scores).mean()
    std_scores = pd.DataFrame(scores).std()
    out_col = []
    for i in range(len(mean_scores)):
        out_col.append((f"%0.3f (+/- %0.3f)" % (mean_scores[i], std_scores[i])))
    return pd.Series(data=out_col, index=mean_scores.index)


# make a scorer function that we can pass into cross-validation
def mape(true, pred):
    return 100.0 * np.mean(np.abs((pred - true) / true))


mape_scorer = make_scorer(mape, greater_is_better=False)


scoring_metrics = {
    "neg_RMSE": "neg_root_mean_squared_error",
    "r2": "r2",
    "mape": mape_scorer,
}


models = {
    "Baseline": make_pipeline(preprocessor, DummyRegressor()),
    "Ridge": make_pipeline(preprocessor, Ridge(random_state=123)),
    "Lasso": make_pipeline(preprocessor, Lasso(random_state=123)),
    "Decision Tree": make_pipeline(
        preprocessor, DecisionTreeRegressor(random_state=123)
    ),
    "LGBM": make_pipeline(preprocessor, LGBMRegressor(random_state=123)),
    "CatBoost": make_pipeline(
        preprocessor, CatBoostRegressor(verbose=0, random_state=123)
    ),
    "Support Vector": make_pipeline(preprocessor, SVR(epsilon=0.2)),
    "KNN": make_pipeline(
        preprocessor,
        KNeighborsRegressor(n_neighbors=30, weights="uniform", algorithm="auto"),
    ),
}


results = {}  # dictionary to store all the results


# saving the mean cross validation scores in resutls dictionary created previously
for model_name, model in models.items():
    results[model_name] = mean_std_cross_val_scores(
        model,
        X_train,
        y_train,
        scoring=scoring_metrics,
        return_train_score=True,
    )


pd.DataFrame(results).rename(index={"test_neg RMSE": "cv_neg_RMSE","test_r2": "cv_r2","test_mape": "cv_mape",}).drop(["fit_time", "score_time"])


import warnings

warnings.filterwarnings("ignore")

cb_param_grid = {
    "catboostregressor__max_depth": [-1, 10, 20, 40, 100, 500],
    "catboostregressor__n_estimators": np.arange(2, 40) * 10,
    "catboostregressor__l2_leaf_reg": np.arange(2, 20),
}

cb_rand = RandomizedSearchCV(
    models["CatBoost"],
    cb_param_grid,
    n_iter=20,
    verbose=1,
    n_jobs=-1,
    random_state=123,
    return_train_score=True,
)
cb_random_search = cb_rand.fit(X_train, y_train)


print("Tuned hyper-params for CatBoost:\n", cb_random_search.best_params_)


pipe_tuned_cb = make_pipeline(
    preprocessor, CatBoostRegressor(verbose=0, 
                                    n_estimators=370, 
                                    max_depth = 10,
                                    l2_leaf_reg = 10,
                                    random_state=123)
)

results['Tuned CatBoost'] = mean_std_cross_val_scores(
    pipe_tuned_cb,X_train,y_train,scoring=scoring_metrics,return_train_score=True,
    )


df_results = pd.DataFrame(results).rename(
    index={"test_neg_RMSE": "cv neg RMSE",
           "test_r2": "cv r2",
           "test_mape": "cv mape",
           "train_neg_RMSE": "train neg RMSE",
           "train_r2": "train r2", 
           "train_mape": "train mape"}).drop(["fit_time", "score_time"])
df_results


df_results.to_csv
dfi.export(df_results, "../results/df_results.png")


# Fit the preprocessor first
preprocessor.fit(X_train, y_train)

# Get all feature names
feature_names = preprocessor.get_feature_names_out()

# Create encoded train data
X_train_enc = pd.DataFrame(
    data=preprocessor.transform(X_train),
    columns=feature_names,
    index=X_train.index,
)
X_train_enc.head()

# Fit regressor model using LGBMRegressor
pipe_cb = make_pipeline(preprocessor, CatBoostRegressor(verbose=0, random_state=123))
pipe_cb.fit(X_train, y_train)

# Create feature importance visualization using shap
explainer = shap.TreeExplainer(pipe_cb.named_steps["catboostregressor"])
train_shap_values = explainer.shap_values(X_train_enc)
shap.summary_plot(train_shap_values, X_train_enc, plot_type="bar", show=False)
plt.savefig("../results/shap_bar.svg", bbox_inches="tight")


shap.summary_plot(train_shap_values, X_train_enc, show=False)
plt.savefig("../results/shap_dots.svg", bbox_inches="tight")


# Predict on the test data and report scores
y_pred = pipe_cb.predict(X_test)
test_scores = {"metrics": list(scoring_metrics.keys()), "best model-lgbm": []}
test_scores["best model-lgbm"].append(
    -1 * mean_squared_error(y_test, y_pred, squared=False)
)
test_scores["best model-lgbm"].append(r2_score(y_test, y_pred))
test_scores["best model-lgbm"].append(mape(y_test, y_pred))
pd.DataFrame(test_scores).set_index("metrics")


# Create encoded test data
X_test_enc = pd.DataFrame(
    data=preprocessor.transform(X_test),
    columns=feature_names,
    index=X_test.index,
)


# We are only extracting shapely values for the first 5 test examples for speed.
test_lgbm_shap_values = explainer.shap_values(X_test_enc[:5])

# Round off feature values for better display
X_train_enc = X_train_enc.round(3)
X_test_enc = X_test_enc.round(3)

# An example
shap.force_plot(
    explainer.expected_value,
    test_lgbm_shap_values[4],
    X_test_enc.iloc[4, :],
    matplotlib=True,
    show=False,
)
plt.savefig("../results/shap_test_force.svg", bbox_inches="tight")


print("True popularity for case one:", y_test.iloc[4])


X_prediction = pd.read_csv("../data/processed/prediction.csv").drop(columns=["Unnamed: 0"])
X_prediction.shape


X_prediction.describe()


y_prediction = pipe_cb.predict(X_prediction)
X_prediction['prediction_bloom_doy'] = y_prediction


X_prediction.head()


final_prediction = round(X_prediction.pivot(index="year", columns="city", values="prediction_bloom_doy"))
for i in final_prediction.columns:
    try:
        final_prediction[[i]] = final_prediction[[i]].astype(int)
    except:
        pass


final_prediction


final_prediction.to_csv('../cherry-predictions.csv')



