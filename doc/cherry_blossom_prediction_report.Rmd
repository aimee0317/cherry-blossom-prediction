---
title: "Cherry Blossom Prediction"
author: "Amelia Tang, Alex Yinan Guo, Nick Lisheng Mao"
date: '2022/02/23 (updated: `r Sys.Date()`)'
output:
  html_document:
    toc: yes
  github_document:
    toc: yes
always_allow_html: yes
bibliography: cherry_blossom_reference.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(kableExtra)
library(tidyverse)
library(scales)
```

```{r read_test_score, include=FALSE}
#test_score <- read_csv("xxx") 
``` 

## Summary

TBU (Amelia)

## Introduction

In recent years, many studies have implemented machine learning techniques to study topics, traditionally covered by phenology models, in agronomy and forestry. In 2021, a study carried out by the Research and Innovation Centre Techniek in the Netherlands forecast the sap flow of cherry tomatoes in a greenhouse leveraging several supervised machine learning algorithms including linear models, such as linear regression (LR), least absolute shrinkage and selection operator (LASSO), elastic net regression (ENR), distance-based algorithms, such as support vector regression (SVR), and tree-based algorithms, such as random forest (RF), gradient boosting (GB) and decision tree (DT). Among all the models, Random forest performed the best, achieving an $R^2$ of $0.808$. Meanwhile, a 2020 study published in Ecological Informatics utilized an unsupervised machine learning technique, self-organizing maps (SOM), to predict peak bloom dates of Yashino cherry trees. However, the unsupervised machine learning models failed to deliver better results than a process-based phenology model did. 

In our project, we built multiple supervised learning models using popular algorithms for predictions, including linear least squares with L2 regularization (Ridge), least absolute shrinkage and selection operator (LASSO), support vector regression (SVR), k-nearest neighbors (KNN), decision tree (DT), categorical boosting (CatBoost), extreme gradient boosting (XGBoost) and Light Gradient Boosting Machine (LGBM). In addition, we implemented a novel strategy and proposed a model leveraging both supervised and unsupervised learning based on K-means clustering (Kmeans) and support vector regression (SVR). After comparing the performances, we constructed a final model using xxx.        

## Methods
### Data Collection and Processing 

Data Collection (Nick)
Processing (Nick)
Imputation (Amelia)

We have xxx lines of missing data after applying a filter to extract all the data after the year 1950. A simplistic imputing technique that would fill in the missing data using median, mean or the most frequent values would not provide an accurate picture in our case. Each feature in the data set reflected information for each city and we had imbalanced amounts of data for each city. For instance, we had xxx lines of data for city xxx and only xxx lines of data for city xxx. Therefore, we used K-Nearest Neighbors (KNN) algorithm for missing data imputation. This imputation technique identified x rows in the data set that were similar and treated them as neighbors to impute missing values. We decided to weight each neighbor by the inverse of the distance so that the closest neighbors would have the greatest influence.         

### Exploratory Data Analysis

TBU - (Alex) 
For the weather data - treat as time series 
For boy - regression 

```{r target, fig.align = 'center', echo=FALSE, fig.cap="Figure 1. Target Distribution", out.width = '40%'}
#knitr::include_graphics("xxx")
```

### Model Selection
- Linear regression (Alex)
- Tree based (Alex)
- distance-based (Amelia)
- Unsupervised Kmeans + SVR (Amelia)
- Hyper parameter tuning (Alex)

### Forecasting 
Weather data (nick)

## Results & Discussion
results (Amelia)
discussion (limitations - more data, try different algorithms etc.)

The missing data imputation method using KNN is susceptible to scaling. 
Try K-means + Gradient boosting? 

# References
TBU 